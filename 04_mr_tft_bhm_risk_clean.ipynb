{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jessietbl/aviation-scsirisk-showcase/blob/main/04_mr_tft_bhm_risk_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RurtId6KvANJ"
      },
      "source": [
        "# MR-TFT Showcase (Lightweight, Dummy Data)\n",
        "\n",
        "This mini notebook uses **dummy monthly SCSI + revenue** and a tiny baseline model\n",
        "to demonstrate the pipeline without GPU or heavy installs.\n",
        "\n",
        "Files expected in `/content/data` (already included in the repo ZIP):\n",
        "- `final_data_with_scsi.csv` (Date, SCSI)\n",
        "- `monthly_revenue_piecewise_pchip.csv` (Date, revenue_RM_monthly)\n",
        "- `monthly_posteriors.csv` (Date, p_revenue)"
      ],
      "id": "RurtId6KvANJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 0 — helper module (lightweight; no OCR/GPUs required)\n",
        "mkdir -p src\n",
        "cat > src/mrtft_bhm.py <<'PY'\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "mrtft_bhm.py — tiny utilities for an MR-TFT + BHM showcase.\n",
        "\n",
        "What’s inside (light + robust):\n",
        "- Monthly I/O helpers (date detection, monthly grid, safe merges)\n",
        "- Posterior handling (attach/engineer p_revenue)\n",
        "- Fourier + multi-resolution features (lags/rolling stats)\n",
        "- Minimal TFT dataset scaffolding (optional)\n",
        "- Simple metrics + Bayesian VaR\n",
        "\n",
        "This demo stays light: no OCR; if you want OCR+BHM later, you can extend.\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "import os, re\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, List, Dict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------------------\n",
        "# Monthly I/O helpers\n",
        "# ---------------------------\n",
        "def _best_date_column(df: pd.DataFrame) -> str:\n",
        "    for k in (\"Date\",\"date\",\"month_end\",\"month\",\"Month\",\"period\",\"timestamp\",\"Unnamed: 0\",\"unnamed: 0\"):\n",
        "        if k in df.columns: return k\n",
        "    for c in df.columns:\n",
        "        s = pd.to_datetime(df[c], errors=\"coerce\")\n",
        "        if s.notna().mean() > 0.6: return c\n",
        "    raise ValueError(\"No date-like column found.\")\n",
        "\n",
        "def _coerce_to_month(x) -> pd.Series:\n",
        "    s = pd.to_datetime(x, errors=\"coerce\")\n",
        "    return s.dt.to_period(\"M\").dt.to_timestamp()\n",
        "\n",
        "def ensure_monthly(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if \"Date\" not in df.columns:\n",
        "        raise KeyError(\"ensure_monthly expects a 'Date' column.\")\n",
        "    d = df.copy()\n",
        "    d[\"Date\"] = _coerce_to_month(d[\"Date\"])\n",
        "    d = d.dropna(subset=[\"Date\"]).sort_values(\"Date\")\n",
        "    if d[\"Date\"].duplicated().any():\n",
        "        num = d.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        non = [c for c in d.columns if c not in set(num + [\"Date\"])]\n",
        "        agg = {**{c: \"mean\" for c in num}, **{c: \"first\" for c in non}}\n",
        "        d = d.groupby(\"Date\", as_index=False).agg(agg)\n",
        "    if d.empty: return d\n",
        "    idx = pd.date_range(d[\"Date\"].min(), d[\"Date\"].max(), freq=\"MS\")\n",
        "    d = d.set_index(\"Date\").reindex(idx)\n",
        "    d.index.name = \"Date\"\n",
        "    return d.reset_index()\n",
        "\n",
        "def merge_monthly_left(left: pd.DataFrame, right: pd.DataFrame | None) -> pd.DataFrame:\n",
        "    L = left.copy()\n",
        "    if \"Date\" not in L.columns:\n",
        "        L = L.rename(columns={_best_date_column(L): \"Date\"})\n",
        "    L = ensure_monthly(L)\n",
        "    if right is None or right.empty:\n",
        "        return L\n",
        "    R = right.copy()\n",
        "    if \"Date\" not in R.columns:\n",
        "        R = R.rename(columns={_best_date_column(R): \"Date\"})\n",
        "    R = ensure_monthly(R)\n",
        "    return L.merge(R, on=\"Date\", how=\"left\")\n",
        "\n",
        "def detect_columns(df: pd.DataFrame,\n",
        "                   scsi_candidates=(\"SCSI\",\"scsi\",\"y\",\"target\",\"index\",\"value\"),\n",
        "                   rev_candidates=(\"revenue\",\"revenue_rm_monthly\",\"rev\",\"monthly_revenue\")) -> Dict[str,str|None]:\n",
        "    low = {c.lower(): c for c in df.columns}\n",
        "    def pick(cands):\n",
        "        for n in cands:\n",
        "            if n.lower() in low: return low[n.lower()]\n",
        "        return None\n",
        "    return {\"scsi\": pick(scsi_candidates), \"revenue\": pick(rev_candidates)}\n",
        "\n",
        "def load_unified_monthly(path_scsi: str,\n",
        "                         path_revenue: Optional[str] = None,\n",
        "                         scsi_name: Optional[str] = None) -> pd.DataFrame:\n",
        "    scsi_raw = pd.read_csv(path_scsi)\n",
        "    scsi_raw = scsi_raw.rename(columns={_best_date_column(scsi_raw): \"Date\"})\n",
        "    scsi_raw[\"Date\"] = _coerce_to_month(scsi_raw[\"Date\"])\n",
        "    scsi_col = scsi_name or detect_columns(scsi_raw).get(\"scsi\") or \"SCSI\"\n",
        "    scsi_raw[scsi_col] = pd.to_numeric(scsi_raw[scsi_col], errors=\"coerce\")\n",
        "    scsi = ensure_monthly(scsi_raw[[\"Date\", scsi_col]].rename(columns={scsi_col: \"SCSI\"}))\n",
        "\n",
        "    revenue = pd.DataFrame(columns=[\"Date\",\"revenue\"])\n",
        "    if path_revenue and os.path.exists(path_revenue):\n",
        "        rev_raw = pd.read_csv(path_revenue)\n",
        "        rev_raw = rev_raw.rename(columns={_best_date_column(rev_raw): \"Date\"})\n",
        "        rev_raw[\"Date\"] = _coerce_to_month(rev_raw[\"Date\"])\n",
        "        rev_col = next((c for c in rev_raw.columns if c!=\"Date\" and\n",
        "                        pd.to_numeric(rev_raw[c], errors=\"coerce\").notna().mean()>0.6), None)\n",
        "        if rev_col:\n",
        "            revenue = ensure_monthly(rev_raw[[\"Date\", rev_col]].rename(columns={rev_col: \"revenue\"}))\n",
        "\n",
        "    d = merge_monthly_left(scsi, revenue)\n",
        "    for c in (\"SCSI\",\"revenue\"):\n",
        "        if c in d.columns:\n",
        "            s = pd.to_numeric(d[c], errors=\"coerce\")\n",
        "            try: d[c] = s.interpolate(\"time\").ffill().bfill()\n",
        "            except: d[c] = s.interpolate().ffill().bfill()\n",
        "    return d\n",
        "\n",
        "# ---------------------------\n",
        "# Posterior features (p_revenue)\n",
        "# ---------------------------\n",
        "def _ensure_posterior_engineered(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    if \"p_revenue\" not in out.columns: out[\"p_revenue\"] = 0.33\n",
        "    out[\"p_revenue\"] = pd.to_numeric(out[\"p_revenue\"], errors=\"coerce\").clip(0.05, 0.95)\n",
        "    out[\"p_revenue\"] = out[\"p_revenue\"].ewm(alpha=0.5, adjust=False).mean()\n",
        "    for L in (1,3,6):\n",
        "        c = f\"p_revenue_lag{L}\"\n",
        "        if c not in out.columns: out[c] = out[\"p_revenue\"].shift(L)\n",
        "    for W in (3,6):\n",
        "        c = f\"p_revenue_rm{W}\"\n",
        "        if c not in out.columns: out[c] = out[\"p_revenue\"].rolling(W, min_periods=1).mean()\n",
        "    eng = [c for c in out.columns if c.startswith(\"p_revenue_\")]\n",
        "    if eng: out[eng] = out[eng].ffill().bfill()\n",
        "    return out\n",
        "\n",
        "def attach_posterior_safe(df: pd.DataFrame, monthly_csv: Optional[str] = None,\n",
        "                          default: float = 0.33, time_col: str = \"Date\",\n",
        "                          scsi_col: Optional[str] = None) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    if time_col not in out.columns:\n",
        "        out = out.rename(columns={_best_date_column(out): time_col})\n",
        "    out[time_col] = _coerce_to_month(out[time_col])\n",
        "    if \"p_revenue\" not in out.columns: out[\"p_revenue\"] = np.nan\n",
        "    if monthly_csv and os.path.exists(monthly_csv):\n",
        "        p = pd.read_csv(monthly_csv)\n",
        "        if \"Date\" not in p.columns:\n",
        "            for c in p.columns:\n",
        "                if \"date\" in c.lower() or \"month\" in c.lower(): p = p.rename(columns={c:\"Date\"}); break\n",
        "        p[\"Date\"] = _coerce_to_month(p[\"Date\"])\n",
        "        prob = next((k for k in (\"p_revenue\",\"posterior\",\"p_rev\",\"P_Revenue\") if k in p.columns), None)\n",
        "        if prob:\n",
        "            p = p[[\"Date\", prob]].rename(columns={prob:\"p_revenue\"})\n",
        "            p[\"p_revenue\"] = pd.to_numeric(p[\"p_revenue\"], errors=\"coerce\")\n",
        "            p = p.dropna(subset=[\"Date\"]).groupby(\"Date\", as_index=False)[\"p_revenue\"].mean()\n",
        "            out = out.merge(p, left_on=time_col, right_on=\"Date\", how=\"left\", suffixes=(\"\", \"_post\"))\n",
        "            out[\"p_revenue\"] = out[\"p_revenue\"].combine_first(out[\"p_revenue_post\"])\n",
        "            out.drop(columns=[c for c in (\"Date\",\"p_revenue_post\") if c in out.columns], inplace=True, errors=\"ignore\")\n",
        "    out[\"p_revenue\"] = pd.to_numeric(out[\"p_revenue\"], errors=\"coerce\").fillna(default).clip(0,1)\n",
        "    return _ensure_posterior_engineered(out)\n",
        "\n",
        "# ---------------------------\n",
        "# Fourier + multi-resolution features\n",
        "# ---------------------------\n",
        "def add_fourier_terms(df: pd.DataFrame, time_col=\"Date\", K=3, period=12) -> pd.DataFrame:\n",
        "    d = df.copy()\n",
        "    d[time_col] = pd.to_datetime(d[time_col], errors=\"coerce\")\n",
        "    d = d.dropna(subset=[time_col]).sort_values(time_col).reset_index(drop=True)\n",
        "    m_idx = (d[time_col].dt.year*12 + d[time_col].dt.month) - (d[time_col].dt.year.min()*12 + d[time_col].dt.month.min())\n",
        "    m_idx = m_idx.astype(float)\n",
        "    for k in range(1, K+1):\n",
        "        ang = 2*np.pi*k*(m_idx/float(period))\n",
        "        d[f\"m_sin_{k}\"] = np.sin(ang); d[f\"m_cos_{k}\"] = np.cos(ang)\n",
        "    return d\n",
        "\n",
        "def create_multi_resolution_features(df: pd.DataFrame, time_col=\"Date\", target=\"SCSI\",\n",
        "                                     mr_windows=(3,6,12), lag_features=(1,3,6,12)) -> Tuple[pd.DataFrame, List[str]]:\n",
        "    d = df.copy()\n",
        "    d[time_col] = pd.to_datetime(d[time_col], errors=\"coerce\")\n",
        "    d = d.dropna(subset=[time_col]).sort_values(time_col).reset_index(drop=True).set_index(time_col)\n",
        "    y = pd.to_numeric(d[target], errors=\"coerce\")\n",
        "    # yoy\n",
        "    d[f\"{target}_yoy\"] = y - y.shift(12)\n",
        "    d[f\"{target}_yoy_abs\"] = (y - y.shift(12)).abs()\n",
        "    mr = sorted(set(list(mr_windows)+[3,6,12])); lags = sorted(set(list(lag_features)+[1,3,6,12]))\n",
        "    for w in mr:\n",
        "        d[f\"{target}_ma_{w}\"] = y.rolling(w, min_periods=1).mean()\n",
        "        d[f\"{target}_std_{w}\"] = y.rolling(w, min_periods=1).std().fillna(0.0)\n",
        "        d[f\"{target}_range_{w}\"] = y.rolling(w, min_periods=1).max() - y.rolling(w, min_periods=1).min()\n",
        "        d[f\"{target}_ewm_{w}\"] = y.ewm(span=w, adjust=False, min_periods=1).mean()\n",
        "    for L in lags:\n",
        "        d[f\"{target}_lag_{L}\"] = y.shift(L)\n",
        "        d[f\"{target}_diff_{L}\"] = y.diff(L)\n",
        "        pc = y.pct_change(L); d[f\"{target}_pct_change_{L}\"] = pc.replace([np.inf,-np.inf], np.nan)\n",
        "        if L<=6: d[f\"{target}_lag_{L}_ma3\"] = d[f\"{target}_lag_{L}\"].rolling(3, min_periods=1).mean()\n",
        "    d[\"month\"] = d.index.month; d[\"quarter\"] = d.index.quarter\n",
        "    d[\"month_sin\"] = np.sin(2*np.pi*d[\"month\"]/12.0); d[\"month_cos\"] = np.cos(2*np.pi*d[\"month\"]/12.0)\n",
        "    d[\"quarter_sin\"] = np.sin(2*np.pi*d[\"quarter\"]/4.0); d[\"quarter_cos\"] = np.cos(2*np.pi*d[\"quarter\"]/4.0)\n",
        "    d[\"time_trend\"] = np.arange(len(d), dtype=int)\n",
        "    d = d.reset_index()\n",
        "    num = d.select_dtypes(include=[np.number]).columns\n",
        "    d[num] = d[num].replace([np.inf,-np.inf], np.nan).ffill().bfill().fillna(0.0)\n",
        "    unknown_allow = (\n",
        "        [f\"{target}_lag_{L}\" for L in (1,3,6,12)] +\n",
        "        [f\"{target}_ma_{w}\" for w in (3,6,12)] +\n",
        "        [f\"{target}_ewm_{w}\" for w in (3,6,12)] +\n",
        "        [f\"{target}_diff_{L}\" for L in (1,3,6,12)] +\n",
        "        [f\"{target}_pct_change_{L}\" for L in (1,3,6,12)] +\n",
        "        [f\"{target}_yoy\", f\"{target}_yoy_abs\"]\n",
        "    )\n",
        "    unknown_allow = [c for c in unknown_allow if c in d.columns]\n",
        "    return d, unknown_allow\n",
        "\n",
        "# ---------------------------\n",
        "# Simple metrics + Bayesian VaR\n",
        "# ---------------------------\n",
        "def eval_rmse_mae_r2(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true, float).reshape(-1)\n",
        "    y_pred = np.asarray(y_pred, float).reshape(-1)\n",
        "    rmse = float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
        "    mae  = float(np.mean(np.abs(y_true - y_pred)))\n",
        "    denom = np.sum((y_true - np.nanmean(y_true))**2)\n",
        "    r2 = float(\"nan\") if denom<=1e-12 else 1.0 - float(np.nansum((y_true - y_pred)**2))/denom\n",
        "    return rmse, mae, r2\n",
        "\n",
        "def _normalize_posterior_df(posterior_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    p = posterior_df.copy()\n",
        "    if \"Date\" not in p.columns:\n",
        "        for c in p.columns:\n",
        "            if \"date\" in c.lower() or \"month\" in c.lower(): p = p.rename(columns={c:\"Date\"}); break\n",
        "    p[\"Date\"] = _coerce_to_month(p[\"Date\"])\n",
        "    pv = next((k for k in (\"p_revenue\",\"posterior\",\"pRev\",\"p_rev\") if k in p.columns), None)\n",
        "    if pv is None: raise KeyError(\"Posterior frame missing probability column.\")\n",
        "    p[pv] = pd.to_numeric(p[pv], errors=\"coerce\")\n",
        "    return p.dropna(subset=[\"Date\"]).groupby(\"Date\", as_index=False)[pv].mean().rename(columns={pv:\"p_revenue\"})\n",
        "\n",
        "def _ensure_p_revenue_col(preds_df, posterior_df, time_col=\"Date\", fallback=0.33):\n",
        "    preds = preds_df.copy(); preds[time_col] = _coerce_to_month(preds[time_col])\n",
        "    post = _normalize_posterior_df(posterior_df)\n",
        "    m = preds.merge(post, left_on=time_col, right_on=\"Date\", how=\"left\", suffixes=(\"\", \"_post\"))\n",
        "    if \"Date\" in m.columns and time_col!=\"Date\": m.drop(columns=[\"Date\"], inplace=True, errors=\"ignore\")\n",
        "    if \"p_revenue\" not in m.columns: m[\"p_revenue\"] = np.nan\n",
        "    if \"p_revenue_post\" in m.columns:\n",
        "        m[\"p_revenue\"] = m[\"p_revenue\"].combine_first(m[\"p_revenue_post\"]); m.drop(columns=[\"p_revenue_post\"], inplace=True)\n",
        "    m[\"p_revenue\"] = pd.to_numeric(m[\"p_revenue\"], errors=\"coerce\").ffill().bfill().fillna(float(fallback)).clip(1e-6,1-1e-6)\n",
        "    return m\n",
        "\n",
        "def calculate_bayesian_var(posterior_df: pd.DataFrame, predictions_df: pd.DataFrame,\n",
        "                           kappa: float = 100.0, flat_revenue: float = 1_000_000.0,\n",
        "                           full_df: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
        "    from scipy.stats import beta\n",
        "    preds = predictions_df.copy(); preds[\"Date\"] = _coerce_to_month(preds[\"Date\"])\n",
        "    if \"y_pred\" not in preds.columns:\n",
        "        for c in (\"pred\",\"yhat\",\"y_pred_cal\",\"yhat_smoothed\"):\n",
        "            if c in preds.columns: preds = preds.rename(columns={c:\"y_pred\"}); break\n",
        "    if \"y_pred\" not in preds.columns: raise KeyError(\"Predictions need a 'y_pred' column (or alias).\")\n",
        "    merged = _ensure_p_revenue_col(preds, posterior_df, time_col=\"Date\", fallback=0.33)\n",
        "    if full_df is not None and \"revenue\" in full_df.columns:\n",
        "        r = full_df[[\"Date\",\"revenue\"]].copy(); r[\"Date\"] = _coerce_to_month(r[\"Date\"])\n",
        "        merged = merged.merge(r, on=\"Date\", how=\"left\")\n",
        "    rows = []\n",
        "    for _, r in merged.iterrows():\n",
        "        p = float(r[\"p_revenue\"]); a=max(p*kappa,1e-6); b=max((1-p)*kappa,1e-6)\n",
        "        rev = float(r[\"revenue\"]) if \"revenue\" in r and pd.notna(r[\"revenue\"]) else flat_revenue\n",
        "        scsi_factor = 1.0 + float(r[\"y_pred\"])/10.0\n",
        "        for q in (0.05,0.50,0.95):\n",
        "            val = float(beta.ppf(q,a,b))*rev*scsi_factor\n",
        "            rows.append({\"Date\": r[\"Date\"], f\"VaR_{int(q*100)}\": val})\n",
        "    out = pd.DataFrame(rows).groupby(\"Date\", as_index=False).mean().sort_values(\"Date\")\n",
        "    return out\n",
        "\n",
        "# ---------------------------\n",
        "# (Optional) minimal TFT dataloaders for later\n",
        "# ---------------------------\n",
        "def build_datasets_with_tail(train_df, val_df, test_df, *,\n",
        "                             target=\"SCSI\", time_col=\"Date\", group_col=\"scsi_series\",\n",
        "                             enc_len=12, pred_len=1, add_target_scales=True, allow_missing_timesteps=True, normalizer=None):\n",
        "    from pytorch_forecasting import TimeSeriesDataSet\n",
        "    # train\n",
        "    tr = train_df.copy()\n",
        "    if group_col not in tr.columns: tr[group_col] = \"scsi_series\"\n",
        "    tr[time_col] = _coerce_to_month(tr[time_col])\n",
        "    tr[target] = pd.to_numeric(tr[target], errors=\"coerce\")\n",
        "    tr[\"time_idx\"] = (tr.groupby(group_col)[time_col].rank(method=\"first\").astype(int) - 1)\n",
        "    from pytorch_forecasting.data import GroupNormalizer\n",
        "    norm = normalizer or GroupNormalizer(groups=[group_col])\n",
        "    ds_tr = TimeSeriesDataSet(\n",
        "        tr, time_idx=\"time_idx\", target=target, group_ids=[group_col],\n",
        "        min_encoder_length=max(1, min(6, enc_len)), max_encoder_length=int(enc_len),\n",
        "        min_prediction_length=int(pred_len), max_prediction_length=int(pred_len),\n",
        "        time_varying_known_reals=[c for c in [\"month_sin\",\"month_cos\",\"quarter_sin\",\"quarter_cos\",\"time_trend\",\"p_revenue\"] if c in tr.columns],\n",
        "        time_varying_unknown_reals=[target],\n",
        "        static_categoricals=[group_col], target_normalizer=norm,\n",
        "        allow_missing_timesteps=allow_missing_timesteps, add_relative_time_idx=True,\n",
        "        add_target_scales=bool(add_target_scales), add_encoder_length=True,\n",
        "    )\n",
        "    tail = train_df.sort_values(time_col).tail(min(enc_len, len(train_df)))\n",
        "    ds_va = TimeSeriesDataSet.from_dataset(ds_tr, pd.concat([tail, val_df], ignore_index=True), stop_randomization=True)\n",
        "    ds_te = TimeSeriesDataSet.from_dataset(ds_tr, pd.concat([tail, test_df], ignore_index=True), stop_randomization=True)\n",
        "    return ds_tr, ds_va, ds_te\n",
        "\n",
        "def make_dataloaders(ds_train, ds_val=None, ds_test=None, *, batch_size=1, num_workers=0, shuffle_train=True, pin_memory=True):\n",
        "    def _dl(ds, train):\n",
        "        if ds is None: return None\n",
        "        return ds.to_dataloader(train=train, batch_size=int(batch_size), num_workers=int(num_workers),\n",
        "                                pin_memory=pin_memory, shuffle=(shuffle_train if train else False))\n",
        "    return _dl(ds_train, True), _dl(ds_val, False), _dl(ds_test, False)\n",
        "\n",
        "def tft_predict_last_horizon(model, dataset, batch_size=1) -> np.ndarray:\n",
        "    dl = dataset.to_dataloader(train=False, batch_size=int(batch_size), num_workers=0, pin_memory=True)\n",
        "    raw = model.predict(dl, mode=\"prediction\", return_x=False) if hasattr(model,\"predict\") else model.predict(dl)\n",
        "    arr = np.asarray(raw)\n",
        "    if arr.ndim == 3: return np.median(arr[:,-1,:], axis=1)\n",
        "    if arr.ndim == 2: return arr[:,-1]\n",
        "    return arr.reshape(-1)\n",
        "\n",
        "PY\n"
      ],
      "metadata": {
        "id": "JObV8mgbpU0G"
      },
      "id": "JObV8mgbpU0G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 — imports & paths\n",
        "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from src.mrtft_bhm import (\n",
        "    load_unified_monthly, attach_posterior_safe, add_fourier_terms, create_multi_resolution_features,\n",
        "    eval_rmse_mae_r2, calculate_bayesian_var\n",
        ")\n",
        "\n",
        "DATA_DIR = Path(\"/content/data\")\n",
        "OUT_DIR  = Path(\"/content/out\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SCSI_CSV   = DATA_DIR / \"final_data_with_scsi.csv\"\n",
        "REV_CSV    = DATA_DIR / \"monthly_revenue_piecewise_pchip.csv\"\n",
        "POST_CSV   = DATA_DIR / \"monthly_posteriors.csv\"\n",
        "\n",
        "# Cell 2 — make dummy data if files not present\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "if not SCSI_CSV.exists() or not REV_CSV.exists() or not POST_CSV.exists():\n",
        "    dates = pd.date_range(\"2019-01-01\",\"2024-12-01\",freq=\"MS\")\n",
        "    m = np.arange(len(dates))\n",
        "    rng = np.random.default_rng(7)\n",
        "\n",
        "    scsi = pd.DataFrame({\n",
        "        \"Date\": dates,\n",
        "        \"SCSI\": 50 + 10*np.sin(2*np.pi*m/12) + 3*np.sin(2*np.pi*m/6) + rng.normal(0,1.2,len(m))\n",
        "    })\n",
        "    scsi.to_csv(SCSI_CSV, index=False)\n",
        "\n",
        "    revenue = pd.DataFrame({\n",
        "        \"Date\": dates,\n",
        "        \"revenue_RM_monthly\": 8e6 + 2e6*np.sin(2*np.pi*(m-2)/12) + rng.normal(0,2.5e5,len(m))\n",
        "    })\n",
        "    revenue.to_csv(REV_CSV, index=False)\n",
        "\n",
        "    # seasonal posterior (Q4 > Q1 > Q2 ~= Q3)\n",
        "    base = np.where((pd.to_datetime(dates).month>=10)|(pd.to_datetime(dates).month<=3), 0.36, 0.30)\n",
        "    post = pd.DataFrame({\"Date\": dates, \"p_revenue\": np.clip(base + rng.normal(0,0.02,len(m)), 0.1, 0.9)})\n",
        "    post.to_csv(POST_CSV, index=False)\n",
        "\n",
        "print(\"Data files ready:\", SCSI_CSV.exists(), REV_CSV.exists(), POST_CSV.exists())\n",
        "\n",
        "# Cell 3 — unify (SCSI+Revenue) and attach posterior\n",
        "df = load_unified_monthly(str(SCSI_CSV), str(REV_CSV))\n",
        "df = attach_posterior_safe(df, monthly_csv=str(POST_CSV), default=0.33)\n",
        "\n",
        "# Add features\n",
        "df = add_fourier_terms(df, time_col=\"Date\", K=3, period=12)\n",
        "df_feat, _ = create_multi_resolution_features(df, time_col=\"Date\", target=\"SCSI\")\n",
        "\n",
        "df_feat.head()\n"
      ],
      "metadata": {
        "id": "g2K6G57bpYYG"
      },
      "id": "g2K6G57bpYYG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 — split & seasonal-naive baseline (12-month lag)\n",
        "train = df_feat[df_feat[\"Date\"].dt.year <= 2022].copy()\n",
        "test  = df_feat[df_feat[\"Date\"].dt.year == 2023].copy()\n",
        "\n",
        "def seasonal_naive(train_df, test_df, col=\"SCSI\", season=12):\n",
        "    tr = train_df[[\"Date\",col]].dropna().set_index(\"Date\")[col]\n",
        "    preds = []\n",
        "    for d in test_df[\"Date\"]:\n",
        "        lag = d - pd.DateOffset(months=season)\n",
        "        preds.append(float(tr.get(lag, tr.mean())))\n",
        "    return np.array(preds, float)\n",
        "\n",
        "y_true = test[\"SCSI\"].to_numpy(float)\n",
        "y_pred = seasonal_naive(train, test, \"SCSI\", 12)\n",
        "\n",
        "rmse, mae, r2 = eval_rmse_mae_r2(y_true, y_pred)\n",
        "print(f\"Baseline metrics  •  RMSE={rmse:.3f}  MAE={mae:.3f}  R2={r2:.3f}\")\n",
        "\n",
        "# Save CSVs\n",
        "pred_df = test[[\"Date\"]].copy()\n",
        "pred_df[\"y_true\"] = y_true\n",
        "pred_df[\"y_pred\"] = y_pred\n",
        "pred_df.to_csv(OUT_DIR/\"showcase_pred_vs_actual.csv\", index=False)\n",
        "\n",
        "pd.DataFrame([{\"metric\":\"RMSE\",\"value\":rmse},{\"metric\":\"MAE\",\"value\":mae},{\"metric\":\"R2\",\"value\":r2}])\\\n",
        "  .to_csv(OUT_DIR/\"showcase_metrics.csv\", index=False)\n",
        "\n",
        "print(\"Saved:\",\n",
        "      OUT_DIR/\"showcase_pred_vs_actual.csv\",\n",
        "      OUT_DIR/\"showcase_metrics.csv\")\n"
      ],
      "metadata": {
        "id": "P_btr6zEpgyc"
      },
      "id": "P_btr6zEpgyc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 — quick plot\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(df[\"Date\"], df[\"SCSI\"], label=\"Actual SCSI\")\n",
        "plt.plot(pred_df[\"Date\"], pred_df[\"y_pred\"], \"--\", label=\"Baseline (seasonal-naive)\")\n",
        "plt.title(\"Showcase: Actual vs Baseline Prediction\"); plt.grid(True, alpha=.3); plt.legend(); plt.show()\n"
      ],
      "metadata": {
        "id": "DvPt3QjLpj2h"
      },
      "id": "DvPt3QjLpj2h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 — Bayesian VaR (optional)\n",
        "var_df = calculate_bayesian_var(\n",
        "    posterior_df = pd.read_csv(POST_CSV),\n",
        "    predictions_df = pred_df[[\"Date\",\"y_pred\"]],\n",
        "    kappa = 80.0,            # prior strength\n",
        "    flat_revenue = 1_000_000,\n",
        "    full_df = df             # uses df['revenue'] if present\n",
        ")\n",
        "var_path = OUT_DIR/\"showcase_bayesian_var.csv\"\n",
        "var_df.to_csv(var_path, index=False)\n",
        "print(\"Saved:\", var_path)\n",
        "var_df.tail(3)\n"
      ],
      "metadata": {
        "id": "en_bebEUpmUK"
      },
      "id": "en_bebEUpmUK",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}